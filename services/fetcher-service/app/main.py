# FILE: ./services/fetcher-service/app/main.py

import schedule
import time
import logging
import os
import requests
import feedparser
from dotenv import load_dotenv
from common.rabbit import RabbitMQClient 
from common.logging_config import setup_logging

load_dotenv()
setup_logging()
logger = logging.getLogger(__name__)

MANAGEMENT_API_URL = os.getenv("MANAGEMENT_API_URL", "http://management-api:8000")
MAX_CONTENT_LENGTH = 10000 # Ø­Ø¯Ø§Ú©Ø«Ø± Ø·ÙˆÙ„ Ù…Ø¬Ø§Ø² Ø¨Ø±Ø§ÛŒ Ù…Ø­ØªÙˆØ§ÛŒ Ø§ØµÙ„ÛŒ

def get_all_sources():
    """Ø§Ø² management-api Ù„ÛŒØ³Øª ØªÙ…Ø§Ù… Ù…Ù†Ø§Ø¨Ø¹ Ø±Ø§ Ø¨Ø§ Ù…Ù†Ø·Ù‚ ØªÙ„Ø§Ø´ Ù…Ø¬Ø¯Ø¯ Ø¯Ø±ÛŒØ§ÙØª Ù…ÛŒâ€ŒÚ©Ù†Ø¯."""
    max_retries = 10
    for i in range(max_retries):
        try:
            response = requests.get(f"{MANAGEMENT_API_URL}/sources")
            response.raise_for_status()
            sources = response.json()
            logger.info(f"Successfully fetched {len(sources)} sources.")
            return sources
        except requests.exceptions.RequestException as e:
            sleep_time = 2 ** i
            logger.warning(f"Could not fetch sources from management-api. Retrying in {sleep_time} seconds... (Attempt {i+1}/{max_retries})")
            time.sleep(sleep_time)
    
    logger.error("Could not connect to management-api after several retries.")
    return []

def is_post_new(post_url: str):
    try:
        response = requests.get(f"{MANAGEMENT_API_URL}/posts/exists", params={"url_original": post_url})
        response.raise_for_status()
        return not response.json().get("exists", True)
    except requests.exceptions.RequestException as e:
        logger.error(f"Could not check post existence. URL: {post_url}. Error: {e}")
        return False

def create_post(post_data: dict):
    """ÛŒÚ© Ø±Ú©ÙˆØ±Ø¯ Ù¾Ø³Øª Ø¬Ø¯ÛŒØ¯ Ø§ÛŒØ¬Ø§Ø¯ Ú©Ø±Ø¯Ù‡ Ùˆ Ø¯Ø± ØµÙˆØ±Øª Ù…ÙˆÙÙ‚ÛŒØªØŒ Ù¾ÛŒØ§Ù…ÛŒ Ø¨Ù‡ RabbitMQ Ø§Ø±Ø³Ø§Ù„ Ù…ÛŒâ€ŒÚ©Ù†Ø¯."""
    try:
        response = requests.post(f"{MANAGEMENT_API_URL}/posts", json=post_data)
        response.raise_for_status()
        new_post = response.json()
        logger.info(f"Successfully created post: {new_post.get('title_original')}")
        
        try:
            with RabbitMQClient() as client:
                message_body = json.dumps({"post_id": new_post.get("id")})
                client.channel.queue_declare(queue='post_created_queue', durable=True)
                client.publish(exchange_name="", routing_key="post_created_queue", body=message_body)
                logger.info(f"Successfully sent creation notification for post_id: {new_post.get('id')}")
        except Exception as e:
            logger.error(f"Failed to send creation notification for post_id: {new_post.get('id')}. Error: {e}")
            
        return new_post
    except requests.exceptions.RequestException as e:
        logger.error(f"Could not create post. Data: {post_data}. Error: {e}")
        return None

def fetch_job():
    logger.info("ğŸš€ Fetcher job started. Looking for new posts...")
    
    sources = get_all_sources()
    if not sources:
        logger.info("No sources found to fetch. Job finished.")
        return

    for source in sources:
        source_id = source.get("id")
        source_url = source.get("url")
        logger.info(f"Fetching source: {source.get('name')} ({source_url})")
        
        feed = feedparser.parse(source_url)
        
        new_posts_found = 0
        for entry in feed.entries:
            post_url = entry.get("link")
            
            if not post_url or not is_post_new(post_url):
                continue

            # --- START: Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ùˆ Ú©ÙˆØªØ§Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù…Ø­ØªÙˆØ§ ---
            content = entry.get("summary", "")
            if len(content) > MAX_CONTENT_LENGTH:
                logger.warning(f"Content from {post_url} is too long ({len(content)} chars). Truncating to {MAX_CONTENT_LENGTH}.")
                content = content[:MAX_CONTENT_LENGTH]
            # --- END: Ø¨Ø®Ø´ Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯Ù‡ ---

            post_data = {
                "source_id": source_id,
                "title_original": entry.get("title", "No Title"),
                "content_original": content, # <-- Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…Ø­ØªÙˆØ§ÛŒ Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ø´Ø¯Ù‡
                "url_original": post_url,
                "image_urls_original": [img.get('href') for img in entry.get('media_content', []) if img.get('href')]
            }
            if create_post(post_data):
                new_posts_found += 1
        
        logger.info(f"Found {new_posts_found} new posts for source '{source.get('name')}'.")
            
    logger.info("âœ… Fetcher job finished.")


def main():
    logger.info("--- ğŸ¤– Fetcher Service Started ---")
    
    schedule.every(5).minutes.do(fetch_job)
    
    logger.info("Initial fetch run will start after a short delay...")
    time.sleep(15)
    fetch_job()
    
    while True:
        schedule.run_pending()
        time.sleep(1)

if __name__ == "__main__":
    main()